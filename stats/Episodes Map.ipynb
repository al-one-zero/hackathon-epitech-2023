{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200bab15-8a57-460b-92f0-52c725f8889b",
   "metadata": {},
   "source": [
    "# Cartographie des diffusions\n",
    "Afin de pouvoir étudier les relartions sémentiques entre les diffusions, on se donne pour but de les visualiser sur le plan.\n",
    "Pour cela, on commence par en génerer un embedding avec un LLM comme *BERT. Cela nous donne une qualification sémantique de la diffusion.\n",
    "On réalise une PCA vers une dimension 50 pour alléger la charge que l'on va mettre sur la réduction de dimension t-SNE que l'on opère enfin pour projeter les diffusions dans le plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef4adba-c414-4384-a072-4e83f177bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as p\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0b7119a-87c2-4ce5-b809-355ff8b68898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    p.Path(\"../dataset\") / \"transcripts\" / \"whisper.csv\",\n",
    "    index_col=\"magnetothequeId\",\n",
    "    nrows=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f71d7a26-cc91-4442-a5e4-362713239eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transcript(json_string: str) -> str:\n",
    "    return \"\".join(s[\"text\"] for s in json.loads(json_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "482711ea-0972-4c84-be29-e55e3face956",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForQuestionAnswering requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForQuestionAnswering\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metalab-ia/camembert-base-squadFR-fquad-piaf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForQuestionAnswering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metalab-ia/camembert-base-squadFR-fquad-piaf\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Projects/hackaton-epitech/venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:1222\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1222\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/hackaton-epitech/venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:1210\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1208\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForQuestionAnswering requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "documents : list[str] = [\n",
    "    \"\".join(s[\"text\"] for s in json.loads(ws))\n",
    "    for ws in df.segments.dropna().tolist()\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c265aed-4f52-445c-b455-87f5bf37147a",
   "metadata": {},
   "source": [
    "Les _attentions_ sont les valeurs d'activation intermediaires de la derniere couche du réseau de neurones dans la variable `model` lorsque l'on lui feed un contenu. Les embeddings d'un texte sont les attentions de `model(transcript)`. Pour finir ce notebook, il faut :\n",
    "  - calculer les attentions de tous les textes a traiter,\n",
    "  - calculer une PCA de ces attentions (avec un objet `sklearn.decomposition.PCA(n_components=50)` puis `fit_transform(attentions)` par exemple)\n",
    "  - calculer une t-SNE un peu comme pour la PCA,\n",
    "  - plot les points sur un graphique `matplotlib`\n",
    "  - colorer par `stationId` ou bien avec un k-Means."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
