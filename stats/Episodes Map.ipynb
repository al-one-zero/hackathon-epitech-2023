{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200bab15-8a57-460b-92f0-52c725f8889b",
   "metadata": {},
   "source": [
    "# Cartographie des diffusions\n",
    "Afin de pouvoir étudier les relartions sémentiques entre les diffusions, on se donne pour but de les visualiser sur le plan.\n",
    "Pour cela, on commence par en génerer un embedding avec un LLM comme *BERT. Cela nous donne une qualification sémantique de la diffusion.\n",
    "On réalise une PCA vers une dimension 50 pour alléger la charge que l'on va mettre sur la réduction de dimension t-SNE que l'on opère enfin pour projeter les diffusions dans le plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef4adba-c414-4384-a072-4e83f177bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as p\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b7119a-87c2-4ce5-b809-355ff8b68898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    p.Path(\"../dataset\") / \"transcripts\" / \"whisper.csv\",\n",
    "    index_col=\"magnetothequeId\",\n",
    "    nrows=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71d7a26-cc91-4442-a5e4-362713239eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transcript(json_string: str) -> str:\n",
    "    return \"\".join(s[\"text\"] for s in json.loads(json_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482711ea-0972-4c84-be29-e55e3face956",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m documents : \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(ws))\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ws \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39msegments\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      4\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForQuestionAnswering\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metalab-ia/camembert-base-squadFR-fquad-piaf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metalab-ia/camembert-base-squadFR-fquad-piaf\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "documents : list[str] = [\n",
    "    \"\".join(s[\"text\"] for s in json.loads(ws))\n",
    "    for ws in df.segments.dropna().tolist()\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"etalab-ia/camembert-base-squadFR-fquad-piaf\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c265aed-4f52-445c-b455-87f5bf37147a",
   "metadata": {},
   "source": [
    "Les _attentions_ sont les valeurs d'activation intermediaires de la derniere couche du réseau de neurones dans la variable `model` lorsque l'on lui feed un contenu. Les embeddings d'un texte sont les attentions de `model(transcript)`. Pour finir ce notebook, il faut :\n",
    "  - calculer les attentions de tous les textes a traiter,\n",
    "  - calculer une PCA de ces attentions (avec un objet `sklearn.decomposition.PCA(n_components=50)` puis `fit_transform(attentions)` par exemple)\n",
    "  - calculer une t-SNE un peu comme pour la PCA,\n",
    "  - plot les points sur un graphique `matplotlib`\n",
    "  - colorer par `stationId` ou bien avec un k-Means."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
